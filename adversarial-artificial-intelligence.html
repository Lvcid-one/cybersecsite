<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Lvcid notes: Adversarial artificial intelligence</title>
    <meta name="description" content="definition for adversarial artificial intelligence">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="">
</head>

<body>
    <article>
        <!-- create hyperlinks for all terms to appropriate page only link the first mention of each term through out -->
        <h1 class="topic">Adversarial artificial intelligence</h1>
        <p class="explanation">
            A technique that manuipulates <a href=""> artificial intelligence (AI)</a> and <a href=""> machine learning
                (ML)</a> technology to conduct attacks more efficiently. More directly it refers to the deliberate
            manipulation of machine learning models by introducing carefully crafted input data. This takes advantage of
            the vulnerabilities in the models' decision-making processes to cause misclassifications or faulty outputs.
        </p>
        <h1 class="how">How it works</h1>
        <p class="explanation">
            Adversarial attacks typically involve making small, imperceptible changes to input data, such as images or
            text, in order to deveive the maching learning model. These changes are designed to exploit the AI's
            weaknesses and lead to incorrect predictions or biased results. By analysing the model's response to these
            modified inputs, attackers can gain insight into the model's internal workings and potential
            vulnerabilities.
        </p>
        <h1 class="why">Why it matters</h1>
        <p class="explanation">
            Defending against adversarial attack in AI is important for ensuring the reliability and trustworthiness of
            machine learning systems. By identifying these vulnerabilities in machine learning models, researchers can
            develop strong defenses and mitigate the risks associated with adverasarial manipulation. Studying
            adversarial attacks can lead to improvements in model training and architecture design, making AI systems
            more secure.
        </p>
        <h1 class="uses">Types of attacks</h1>
        <ul>
            <li>Image classification:
                manipulate images in a way that fools the model into misclassifying them. This has
                implications in area such as autonomous vehicles, security systems, and medical imaging.
            </li>
            <li>Text generation:
                Used to generate deceptive or misleading text that can manipulate <a href="">sentiment analysis</a>
                algorithms, spam filters, or automated content moderation systems
            </li>
            <li>Malware evasion:
                Can be leveraged to design malware that evades detection by antivirus or intrusion detection systems,
                exploiting vulnerabilities in their machine learning-based detection mechanisms.
            </li>
        </ul>
        <h1 class="related-terms">Other related terms and tech: </h1>
        <ul>
            <li>Defensive adversarial learning:
                Techniques and strategies aimed at enhancing the robustness of machine learning models against
                adverarial attacks.
            </li>
            <li>Adversarial examples:
                Inputs specifically designed to intentionally deceive machine learning models.
            </li>
            <li><a href="">Generative Adversarial Networks</a>(GANs):
                A class of machine learning models that consists of a generator and a discriminator network trained in
                an adversarial manner.
            </li>
            <li><a href="">Transfer learning</a>
                A technique that enables the transfer of knowledge from one machine learning task to another, which can
                be vulnerable to adversarial attacks.
            </li>
        </ul>
    </article>
</body>

</html>